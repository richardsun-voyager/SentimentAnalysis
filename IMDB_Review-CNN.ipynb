{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim import utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/IMDB_review_train.csv')\n",
    "test_data = pd.read_csv('data/IMDB_review_test.csv')\n",
    "train_texts = list(train_data.text.values)\n",
    "train_labels = list(train_data.sentiment.values)\n",
    "test_texts = list(test_data.text.values)\n",
    "test_labels = list(test_data.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_DOCUMENT_LENGTH = 800\n",
    "##vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=2) # tensorflow提供的工具，将数据填充为最大长度，默认0填充\n",
    "##train_sent_idx = np.array(list(vocab_processor.fit_transf\n",
    "#                               orm(train_texts)))\n",
    "#test_sent_idx = np.array(list(vocab_processor.transform(test_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_words = len(vocab_processor.vocabulary_)\n",
    "#n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to process....\n",
      "Processing Finished! Timing:  877.151\n",
      "Start to process....\n",
      "Processing Finished! Timing:  875.277\n"
     ]
    }
   ],
   "source": [
    "from tools.text_preprocess import text_clean\n",
    "tc = text_clean(train_texts, is_multiprocess=False)\n",
    "train_processed = tc.proceed()\n",
    "tc = text_clean(test_texts, is_multiprocess=False)\n",
    "test_processed = tc.proceed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/intermediate_data/training.pk', 'wb') as f:\n",
    "    pickle.dump(train_processed, f, -1)\n",
    "with open('data/intermediate_data/testing.pk', 'wb') as f:\n",
    "    pickle.dump(test_processed, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'text': train_processed, 'sentiment':train_labels}\n",
    "temp =  pd.DataFrame(data)\n",
    "temp.to_csv('data/intermediate_data/train_processed.csv', index=False)\n",
    "data = {'text': test_processed, 'sentiment':test_labels}\n",
    "temp =  pd.DataFrame(data)\n",
    "temp.to_csv('data/intermediate_data/test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/intermediate_data/train_processed.csv')\n",
    "test_data = pd.read_csv('data/intermediate_data/train_processed.csv')\n",
    "train_processed = list(train_data.text.values)\n",
    "train_labels = list(train_data.sentiment.values)\n",
    "test_processed = list(test_data.text.values)\n",
    "test_labels = list(test_data.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for item in train_processed:\n",
    "    words.extend(item.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start mapping words to IDs....\n",
      "Processing Finished! Timing:  3.332\n"
     ]
    }
   ],
   "source": [
    "from tools.text_hier_split import sent2words\n",
    "from tools.token_idx_map import token2idx\n",
    "sw_train = sent2words(train_processed)\n",
    "sw_test = sent2words(test_processed)\n",
    "train_sent_words = sw_train.proceed()\n",
    "test_sent_words = sw_test.proceed()\n",
    "ti = token2idx(train_sent_words, 30000)\n",
    "train_sent_idx = ti.proceed()\n",
    "test_sent_idx = ti.map_text_idx(test_sent_words, ignore_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 30001\n"
     ]
    }
   ],
   "source": [
    "n_words = len(ti.get_vocab())\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.sample_generator import generate_samples\n",
    "MAX_DOCUMENT_LENGTH = 800\n",
    "gs_train = generate_samples(train_sent_idx, train_labels, MAX_DOCUMENT_LENGTH)\n",
    "gs_test = generate_samples(test_sent_idx, test_labels, MAX_DOCUMENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = n_words\n",
    "    max_doc_len = MAX_DOCUMENT_LENGTH\n",
    "    label_size = 2\n",
    "    embed_size = 64\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    vocab_size = n_words\n",
    "    max_doc_len = MAX_DOCUMENT_LENGTH\n",
    "    label_size = 2\n",
    "    embed_size = 64\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    vocab_size = n_words\n",
    "    max_doc_len = MAX_DOCUMENT_LENGTH\n",
    "    label_size = 2\n",
    "    embed_size = 64\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_processed)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_processed)/trainConfig.batch_size)\n",
    "remain_num = len(test_processed) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiChannelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "class CNN_Model:\n",
    "    def __init__(self, config, x, y, lengths, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lengths = lengths\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, self.label_size, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        #Create embedding matrix\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            #Batch_size, word_length, embed_size\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "        #Mask those padding words as 0, others as 11\n",
    "        mask = tf.sequence_mask(self.lengths, MAX_DOCUMENT_LENGTH)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        #Duplicate binary values for the embeddings\n",
    "        #Those embeddings for the padding words are masked\n",
    "        #Broadcast embed_size times\n",
    "        mask_transient = tf.tile(mask, [1, self.embed_size])\n",
    "        mask_embed = tf.transpose(tf.reshape(mask_transient, [self.batch_size, \n",
    "                                                              self.embed_size,\n",
    "                                                MAX_DOCUMENT_LENGTH]), [0, 2, 1])\n",
    "        mask_inputs = tf.multiply(mask_embed, inputs)\n",
    "        #Expand the dim to cater to CNN\n",
    "        #Batch_size, word_length, embed_size, 1\n",
    "        intputs_expanded = tf.expand_dims(mask_inputs, -1)\n",
    "        #print(intputs_expanded)\n",
    "        \n",
    "        #Three kinds of convolutional kernels, with kernel size 2, 3, 4\n",
    "        filter_sizes = [2, 3, 4]\n",
    "        num_filters = 128\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                h = tf.layers.conv2d(intputs_expanded, num_filters,\n",
    "                                       kernel_size=(filter_size, self.embed_size),\n",
    "                                       strides=(1, 1), padding='valid',\n",
    "                                        activation=tf.nn.relu)\n",
    "    \n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, filter_size, \n",
    "                           1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                #Swap the last two dimensions\n",
    "                pooled = tf.transpose(pooled, [0, 1, 3, 2])\n",
    "                #Second conv layer\n",
    "                h = tf.layers.conv2d(pooled, num_filters,\n",
    "                                       kernel_size=(filter_size, num_filters),\n",
    "                                       strides=(1, 1), padding='valid',\n",
    "                                        activation=tf.nn.relu)\n",
    "    \n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, MAX_DOCUMENT_LENGTH - 3*filter_size + 3, \n",
    "                           1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "        \n",
    "        \n",
    "                pooled_outputs.append(pooled)\n",
    "            #self.weights.append(W)\n",
    "            \n",
    "        \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        if self.is_training:\n",
    "            h_pool_flat = tf.nn.dropout(h_pool_flat, 0.5)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('output'):\n",
    "            logits = tf.layers.dense(h_pool_flat, self.label_size, \n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "            #weights = tf.get_variable('weights', [num_filters_total, self.label_size], dtype=tf.float32)\n",
    "            #biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_data = tf.placeholder(tf.int32, [trainConfig.batch_size, MAX_DOCUMENT_LENGTH])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_lengths = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model(trainConfig, train_data, train_label, train_lengths)\n",
    "            saver=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        test_data = tf.placeholder(tf.int32, [testConfig.batch_size, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_lengths = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        single_data = tf.placeholder(tf.int32, [singleConfig.batch_size, None])\n",
    "        single_label = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        single_lengths = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model(testConfig, test_data, test_label, test_lengths, False)\n",
    "            single_model = CNN_Model(singleConfig, single_data, single_label, single_lengths, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_train = generate_samples(train_sent_idx, train_labels, MAX_DOCUMENT_LENGTH)\n",
    "gs_test = generate_samples(test_sent_idx, test_labels, MAX_DOCUMENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931\n",
      "Loss: 0.6939\n",
      "Loss: 0.6691\n",
      "Loss: 0.5977\n",
      "Epoch 0 time:21.78\n",
      "Loss: 0.5484\n",
      "Loss: 0.4285\n",
      "Loss: 0.4162\n",
      "Loss: 0.3698\n",
      "Epoch 1 time:21.01\n",
      "Loss: 0.4879\n",
      "Loss: 0.5288\n",
      "Loss: 0.5037\n",
      "Loss: 0.4314\n",
      "Epoch 2 time:21.09\n",
      "Loss: 0.5402\n",
      "Loss: 0.3839\n",
      "Loss: 0.3637\n",
      "Loss: 0.343\n",
      "Epoch 3 time:21.10\n",
      "Loss: 0.307\n",
      "Loss: 0.3807\n",
      "Loss: 0.3091\n",
      "Loss: 0.2428\n",
      "Epoch 4 time:21.25\n",
      "Testing...\n",
      "Test Samples come to an end!\n",
      "Testing Time:6.35\n",
      "0.8894\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 5\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "   \n",
    "    if os.path.exists(\"ckpt_cnn/cnn.ckpt.index\"):\n",
    "        saver.restore(sess, file)\n",
    "    else:\n",
    "         sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths, _ = gs_train.generate_batch(trainConfig.batch_size)\n",
    "            feed_dict = {train_data:x, train_label:y, train_lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        start_time = end_time\n",
    "    saver.save(sess,'ckpt_cnn/cnn.ckpt')\n",
    "    #Calculate Testing Accuracy\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    gs_test = generate_samples(test_sent_idx, test_labels, MAX_DOCUMENT_LENGTH)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths, _ = gs_test.generate_batch(testConfig.batch_size, False)\n",
    "        feed_dict = {test_data:x, test_label:y, test_lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths, _ = gs_test.generate_batch(1, False)\n",
    "        feed_dict = {single_data:x, single_label:y, single_lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_texts))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

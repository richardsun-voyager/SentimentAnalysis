{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/intermediate_data/train_processed.csv')\n",
    "test_data = pd.read_csv('data/intermediate_data/test_processed.csv')\n",
    "train_processed = list(train_data.text.values)\n",
    "train_labels = list(train_data.sentiment.values)\n",
    "test_processed = list(test_data.text.values)\n",
    "test_labels = list(test_data.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/sswe-u.txt'\n",
    "word_emb = {}\n",
    "with open(file) as fi:\n",
    "    for line in fi:\n",
    "        items = line.split()\n",
    "        word_emb[items[0]] = np.array(items[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57881469,  0.92556798,  0.46488839,  0.35405961, -2.63635993,\n",
       "        2.29956794, -0.66394877,  0.61521488, -0.16932181,  0.25359181,\n",
       "        0.52391988,  0.03204051, -0.4096407 , -0.1301239 ,  0.1830793 ,\n",
       "       -0.84529358, -1.40380001, -0.32352889, -1.46755195, -0.46172541,\n",
       "        1.23517299, -1.01539004,  0.89252478,  1.23653102,  1.10636997,\n",
       "        0.94847381, -1.05302501,  0.45638961, -1.52356398, -0.01358143,\n",
       "        0.53842032, -2.00235391,  0.88459599,  1.26996398, -1.64902902,\n",
       "       -0.96611053,  0.04843707, -0.01786563,  1.134794  ,  0.78327078,\n",
       "       -1.52546799, -1.791098  , -0.98401898, -0.1604346 ,  0.29296371,\n",
       "        0.64560997,  2.0015769 ,  1.38100803, -0.74041641,  1.55809605], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137052"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocabulary size\n",
    "len(word_emb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word, vocab_embed):\n",
    "    try:\n",
    "        vec = vocab_embed[word]\n",
    "    except:\n",
    "        vec = word_emb['<unk>']\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent each sentence as the average embeddings of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computer the sent vector based on word vectors\n",
    "train_sent_vecs = []\n",
    "for sent in train_processed:\n",
    "    words = sent.split()\n",
    "    sent_vec = np.zeros(50)*1.0\n",
    "    for word in words:\n",
    "        vec = word2vec(word, word_emb)\n",
    "        sent_vec += vec\n",
    "    train_sent_vecs.append(sent_vec/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_vecs = []\n",
    "for sent in test_processed:\n",
    "    words = sent.split()\n",
    "    sent_vec = np.zeros(50)*1.0\n",
    "    for word in words:\n",
    "        vec = word2vec(word, word_emb)\n",
    "        sent_vec += vec\n",
    "    test_sent_vecs.append(sent_vec/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(train_sent_vecs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77747999999999995"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(train_sent_vecs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77664"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_sent_vecs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_emb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)\n",
    "words = ['good', 'bad', 'nice', 'awesome', 'aweful', 'shit', 'positive']\n",
    "embs = []\n",
    "for w in words:\n",
    "    embs.append(word_emb[w])\n",
    "embs = np.stack(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "num_points = 400\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAANSCAYAAADYmwcgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(15,15))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "\n",
    "#words = [list(ti.get_vocab())[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_lengths = [len(sent.split()) for sent in train_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10.,  129.,  177.,  288.,  464.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(sent_lengths, [0, 25, 50, 75, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computer the sent vector based on word vectors\n",
    "max_word_len = 500\n",
    "train_sent_vecs = []\n",
    "for sent in train_processed:\n",
    "    words = sent.split()\n",
    "    sent_vec = []\n",
    "    for word in words:\n",
    "        vec = word2vec(word, word_emb)\n",
    "        sent_vec.append(vec)\n",
    "    #Cut long sentence\n",
    "    if len(words) > max_word_len:\n",
    "        sent_vec = sent_vec[:max_word_len]\n",
    "    #Pad short sentence\n",
    "    if len(words) < max_word_len:\n",
    "        for _ in np.arange(max_word_len - len(words)):\n",
    "            vec = np.zeros(50)\n",
    "            sent_vec.append(vec)\n",
    "    sent_vec = np.stack(sent_vec)\n",
    "    train_sent_vecs.append(sent_vec)\n",
    "train_sent_vecs = np.stack(train_sent_vecs)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_samples:\n",
    "    '''\n",
    "    Generate samples for training data or testing data\n",
    "    '''\n",
    "    def __init__(self, data, labels, is_training=True):\n",
    "        '''\n",
    "        Args:\n",
    "        data: numpy\n",
    "        labels: numpy\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "        self.index = 0\n",
    "        \n",
    "    def generate_samples(self, sents, labels, batch_size=64):\n",
    "        '''\n",
    "        Select a batch_size of sentences\n",
    "        Transform each sentence into a sequence of embeddings\n",
    "        '''\n",
    "        indice = np.random.choice(len(sents), batch_size)\n",
    "        sents = sents[indice]\n",
    "        labels = labels[indice]\n",
    "        sent_vecs, sent_lens = create_sent_emb(sents)\n",
    "        return sent_vecs, labels, sent_lens\n",
    "    \n",
    "    def create_sent_emb(self, sents):\n",
    "        '''\n",
    "        Create sequences of word embeddings for sentences\n",
    "        '''\n",
    "        sent_vecs = []#A matrix represents a sentence\n",
    "        sent_lens = []#length of each sentence\n",
    "        for sent in sents:\n",
    "            words = sent.split()\n",
    "            sent_lens.append(len(words))\n",
    "            sent_vec = []\n",
    "            for word in words:\n",
    "                vec = word2vec(word, word_emb)\n",
    "                sent_vec.append(vec)\n",
    "            #Cut long sentence\n",
    "            if len(words) > max_word_len:\n",
    "                sent_vec = sent_vec[:max_word_len]\n",
    "            #Pad short sentence\n",
    "            if len(words) < max_word_len:\n",
    "                for _ in np.arange(max_word_len - len(words)):\n",
    "                    vec = np.zeros(50)\n",
    "                    sent_vec.append(vec)\n",
    "            sent_vec = np.stack(sent_vec)\n",
    "            sent_vecs.append(sent_vec)\n",
    "        sent_vecs = np.stack(sent_vecs)\n",
    "        return sent_vecs, sent_lens\n",
    "        \n",
    "    def generate(self, batch_size=64):\n",
    "        if self.is_training:\n",
    "            sent_vecs, sent_labels, lengths = self.generate_samples(self.data, \n",
    "                                                               self.labels,\n",
    "                                                              batch_size)\n",
    "        else:\n",
    "            start = self.index\n",
    "            end = start + batch_size\n",
    "            if end > len(self.data):\n",
    "                print('Out of sample size')\n",
    "                self.index = 0\n",
    "            sents = self.data[start:end]\n",
    "            sent_labels = self.labels[start:end]\n",
    "            sent_vecs, lengths = self.create_sent_emb(sents)\n",
    "            self.index = end\n",
    "        return sent_vecs, sent_labels, lengths\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gs = generate_samples(np.array(train_processed), np.array(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = train_gs.generate(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gs = generate_samples(np.array(test_processed), np.array(test_labels), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = test_gs.generate(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 50\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 50\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 50\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "class CNN_Model:\n",
    "    def __init__(self, config, is_training=True):\n",
    "        self.embed_size = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.is_training = is_training\n",
    "        self.x = tf.placeholder(tf.float32, \n",
    "                                [self.batch_size, self.max_doc_len, \n",
    "                                 self.embed_size])\n",
    "        self.y = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        self.lengths = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, self.label_size, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        #Batch_size, word_length, embed_size\n",
    "        inputs = tf.nn.dropout(self.x, 0.5)\n",
    "        #Mask those padding words as 0, others as 11\n",
    "        #mask = tf.sequence_mask(self.lengths, MAX_DOCUMENT_LENGTH)\n",
    "        #mask = tf.cast(mask, dtype=tf.float32)\n",
    "        #Duplicate binary values for the embeddings\n",
    "        #Those embeddings for the padding words are masked\n",
    "        #Broadcast embed_size times\n",
    "        #mask_transient = tf.tile(mask, [1, self.embed_size])\n",
    "        #mask_embed = tf.transpose(tf.reshape(mask_transient, [self.batch_size, \n",
    "                                                              #self.embed_size,\n",
    "                                                #MAX_DOCUMENT_LENGTH]), [0, 2, 1])\n",
    "        #mask_inputs = tf.multiply(mask_embed, inputs)\n",
    "        #Expand the dim to cater to CNN\n",
    "        #Batch_size, word_length, embed_size, 1\n",
    "        intputs_expanded = tf.expand_dims(inputs, -1)\n",
    "        \n",
    "        #Three kinds of convolutional kernels, with kernel size 2, 3, 4\n",
    "        filter_sizes = [2, 3, 4]\n",
    "        num_filters = 128\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                h = tf.layers.conv2d(intputs_expanded, num_filters,\n",
    "                                       kernel_size=(filter_size, self.embed_size),\n",
    "                                       strides=(1, 1), padding='valid',\n",
    "                                        activation=tf.nn.relu)\n",
    "    \n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.max_doc_len-filter_size+1, \n",
    "                           1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "        \n",
    "                pooled_outputs.append(pooled)\n",
    "            #self.weights.append(W)\n",
    "            \n",
    "        \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        if self.is_training:\n",
    "            h_pool_flat = tf.nn.dropout(h_pool_flat, 0.5)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('output'):\n",
    "            logits = tf.layers.dense(h_pool_flat, self.label_size, \n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "            #weights = tf.get_variable('weights', [num_filters_total, self.label_size], dtype=tf.float32)\n",
    "            #biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model(trainConfig)\n",
    "            saver=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model(testConfig, False)\n",
    "            single_model = CNN_Model(singleConfig, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_processed)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_processed)/testConfig.batch_size)\n",
    "train_chunk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remain_num = len(test_processed) - test_chunk_num * testConfig.batch_size\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt_cnn/cnn.ckpt\n",
      "Loss: 0.5636\n",
      "Loss: 0.5592\n",
      "Loss: 0.5986\n",
      "Loss: 0.6545\n",
      "Epoch 0 time:241.21\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 1\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    if os.path.exists(\"ckpt_cnn/cnn.ckpt.index\"):\n",
    "        saver.restore(sess, file)\n",
    "    else:\n",
    "         sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_gs.generate(trainConfig.batch_size)\n",
    "            feed_dict = {train_model.x:x, train_model.y:y, train_model.lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        \n",
    "    saver.save(sess,'ckpt_cnn/cnn.ckpt')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ckpt_cnn/cnn.ckpt\n",
      "Parameters restored\n",
      "Testing Time:96.63\n",
      "0.7452\n"
     ]
    }
   ],
   "source": [
    "#Calculate Testing Accuracy\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    #saver = tf.train.import_meta_graph('ckpt_cnn/cnn.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('ckpt_cnn/'))\n",
    "    print('Parameters restored')\n",
    "    start_time = time.time()\n",
    "    test_gs = generate_samples(np.array(test_processed), np.array(test_labels), False)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(testConfig.batch_size)\n",
    "        feed_dict = {test_model.x:x, test_model.y:y, test_model.lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(1)\n",
    "        feed_dict = {single_model.x:x, single_model.y:y, \n",
    "                     single_model.lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_processed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
